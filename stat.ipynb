{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Для подсчёта статистики по новым витринам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters coming from airflow\n",
    "capture_type = 'full' #витрина требует сначала полной прогрузки в режиме \"full\" а затем можно переключить на \"increment\"\n",
    "log_fl = None\n",
    "ui_enable = 'false'\n",
    "\n",
    "# project parameters\n",
    "project_name = 'data_completeness'\n",
    "#root_path = '/project'\n",
    "root_path = '/user/mivanovskij'\n",
    "project_path = root_path + '/' + project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark context is not running\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext, SparkSession, HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from datetime import datetime as dttm\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# config processing\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print('Spark context is not running')\n",
    "\n",
    "# context config\n",
    "jar_paths = ['/etc/jdbc/*']\n",
    "conf = (pyspark.SparkConf()\\\n",
    "            .set('spark.dynamicAllocation.maxExecutors','200')\n",
    "            .set('spark.driver.extraClassPath', ':'.join(jar_paths))\n",
    "            .set('spark.executor.extraClassPath', ':'.join(jar_paths))\n",
    "            .set('spark.jars', ','.join(jar_paths))\n",
    "            .set('spark.ui.enabled', ui_enable) #Выключить при установке на регламент\n",
    "            .set('spark.executor.memory','8g')\n",
    "            .set('spark.driver.memory', '8g')\n",
    "            .set('spark.scheduler.mode','FAIR')\n",
    "            .set('spark.driver.maxResultSize', '8g')\n",
    "            .set('spark.rpc.message.maxSize','256')\n",
    "            .set('spark.sql.execution.arrow.enabled', 'true')\n",
    "           )\n",
    "    \n",
    "# Config initialization\n",
    "sc = pyspark.SparkContext(appName=project_name, conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "sql = sqlContext.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logging\n",
    "def log_msg(path, msg, msg_type = 'INFO', process = 'PapermillOperator'):\n",
    "    msg_dttm = '[' + str(dttm.now())[:-3].replace('.', ',') + ']'\n",
    "    if path:\n",
    "        with open(path, 'a') as file:\n",
    "            print(msg_dttm\n",
    "                  , '{' + process + '}'\n",
    "                  , msg_type, '-'\n",
    "                  , msg\n",
    "                  , file = file)\n",
    "    else:\n",
    "        print(msg_dttm\n",
    "                  , '{' + process + '}'\n",
    "                  , msg_type, '-'\n",
    "                  , msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция для вытаскивания индексов из икспасов\n",
    "def xpath_index(s):\n",
    "    s = \"\" if s is None else s\n",
    "    ix = []\n",
    "    for pt in s.split(\"/\"):\n",
    "        a, b = pt.find(\"[\"), pt.find(\"]\")\n",
    "        n = pt[a + 1 : b] if a >= 0 and b >= 0 else 0\n",
    "        try:\n",
    "            n1 = int(n)\n",
    "        except:\n",
    "            n1 = 0\n",
    "        ix.append(f\"{n1:03d}\")\n",
    "    for i in range(len(ix)):\n",
    "        if ix[-1] != \"000\":\n",
    "            break\n",
    "        else:\n",
    "            ix = ix[:-1]\n",
    "    return str.join(\"_\", ix)\n",
    "\n",
    "\n",
    "# register for pyspark synthax\n",
    "udf_xpath_index = F.udf(lambda x: xpath_index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hadoop configuration\n",
    "hadoop = sqlContext._jvm.org.apache.hadoop\n",
    "hdfs = hadoop.fs.FileSystem.get(sqlContext._jsc.hadoopConfiguration())\n",
    "fs = hadoop.fs.FileSystem\n",
    "hadoop_conf = hadoop.conf.Configuration()\n",
    "\n",
    "#Check directory size\n",
    "def get_hdfs_directory_size(input_path):\n",
    "    path = hadoop.fs.Path(input_path)\n",
    "    dir_size = 0\n",
    "    if hdfs.exists(path) != True:\n",
    "        return dir_size\n",
    "    for ffile in fs.get(hadoop_conf).listStatus(path):\n",
    "        dir_size=dir_size+ffile.getLen()\n",
    "    return dir_size\n",
    "\n",
    "#Calculate repartition size\n",
    "def get_repartition_factor(dir_size, compression_rate):\n",
    "    try:\n",
    "        block_size = int(sqlContext._jsc.hadoopConfiguration().get(\"dfs.blocksize\"))\n",
    "        return math.ceil(dir_size/compression_rate/block_size)\n",
    "    except Exception as e:\n",
    "        print('ERROR: ' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consul utils\n",
    "import consul\n",
    "consul_host='srv-bigdata-etl01.mosgorzdrav.local'\n",
    "consul_port='8500'\n",
    "consul=consul.Consul(host=consul_host,port=consul_port)\n",
    "service='vault'\n",
    "\n",
    "if not len(consul.agent.members())>0:\n",
    "     raise ValueError('Consul is not found on {consul_host}:{consul_port}'.format(consul_host=consul_host,consul_port=consul_port))\n",
    "        \n",
    "def list_consul_services(consul=consul):\n",
    "    return list(consul.catalog.services()[1].keys())\n",
    "\n",
    "def get_service_params(service=service,consul=consul):\n",
    "    id, service_info = consul.catalog.service(service)\n",
    "    if len(service_info)==0:\n",
    "        raise ValueError('service {service} is not found'.format(service=service))\n",
    "    index, nodes = consul.health.service(service, passing=True)\n",
    "    if len(nodes)==0:\n",
    "        raise ValueError('No active nodes for service {service}'.format(service=service))\n",
    "    address = nodes[0]['Service']['Address']\n",
    "    port = nodes[0]['Service']['Port']\n",
    "    for x in ['address','port']:\n",
    "        if eval(x) is None or len(str(eval(x)))==0:\n",
    "            raise ValueError('Parameter {x} for service {service} not found in Consul'.format(x=x,service=service))\n",
    "    return address, port\n",
    "\n",
    "def get_db_params(service=service,consul=consul):\n",
    "    id, service_info = consul.catalog.service(service)\n",
    "    if len(service_info)==0:\n",
    "        raise ValueError('service {service} is not found'.format(service=service))\n",
    "    index, nodes = consul.health.service(service, passing=True)\n",
    "    if len(nodes)==0:\n",
    "        raise ValueError('No active nodes for service {service}'.format(service=service))\n",
    "    address = nodes[0]['Service']['Address']\n",
    "    port = nodes[0]['Service']['Port']\n",
    "    database= nodes[0]['Service']['Meta']['service_database_name']\n",
    "    conn_string=nodes[0]['Service']['Meta']['service_rfc1738_with_password_connection_string_template']\n",
    "    for x in ['address','port','database','conn_string']:\n",
    "        if eval(x) is None or len(str(eval(x)))==0:\n",
    "            raise ValueError('Parameter {x} for service {service} not found in Consul'.format(x=x,service=service))\n",
    "    return address, port, database, conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vault utils\n",
    "import hvac\n",
    "from hvac.exceptions import *\n",
    "vault_service='vault'\n",
    "vault_address, vault_port = get_service_params(vault_service)\n",
    "vault_preauth = hvac.Client(url=\"http://{host}:{port}\".format(host=vault_address, port=vault_port),token='')\n",
    "\n",
    "if 'SECRET_TOKEN' in os.environ.keys():\n",
    "    app_vault_token = os.environ['SECRET_TOKEN']\n",
    "else:\n",
    "    secret_id = os.environ['SECRET_ID']\n",
    "    role_id = os.environ['ROLE_ID']\n",
    "    app_vault_token = vault_preauth.auth_approle(secret_id=secret_id, role_id=role_id)\n",
    "    app_vault_token = app_vault_token['auth']['client_token']    \n",
    "    \n",
    "vault = hvac.Client(url=\"http://{host}:{port}\".format(host=vault_address, port=vault_port),token=app_vault_token)\n",
    "\n",
    "def getSecrets(path,mount_point='datalab',vault=vault):\n",
    "    secrets = vault.secrets.kv.read_secret_version(path=path, mount_point=mount_point)['data']['data']\n",
    "    return secrets['username'], secrets['password']\n",
    "\n",
    "def listVaultServices(path='source/',vault=vault,mount_point='datalab'):\n",
    "    return [x.rstrip('/') for x in vault.secrets.kv.v2.list_secrets(path=path,mount_point=mount_point)['data']['keys']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-19 17:11:20,059] {PapermillOperator} INFO - <<< Loading params >>>\n",
      "[2023-07-19 17:11:20,059] {PapermillOperator} INFO - capture_type: full\n",
      "[2023-07-19 17:11:20,059] {PapermillOperator} INFO - project_path: /user/mivanovskij/data_completeness\n",
      "[2023-07-19 17:11:20,059] {PapermillOperator} INFO - log_fl: None\n",
      "[2023-07-19 17:11:20,059] {PapermillOperator} INFO - ui_enable: false\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "log_msg(log_fl, '<<< Loading params >>>')\n",
    "log_msg(log_fl, 'capture_type: ' + capture_type)\n",
    "log_msg(log_fl, 'project_path: ' + project_path)\n",
    "log_msg(log_fl, 'log_fl: ' + str(log_fl))\n",
    "log_msg(log_fl, 'ui_enable: ' + ui_enable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to GP\n",
    "service = 'gp_etl'\n",
    "address, port, database, conn_string = get_db_params(service)\n",
    "path = 'source/{service}/prod/reader'.format(service=service)\n",
    "username, password = getSecrets(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df = sqlContext.read.format('parquet').load(project_path + '/mart_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функции для подсчёта статистики по витрине\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def cnt_ids(x):\n",
    "    total = x.count()\n",
    "    distinct = x.select(countDistinct(\"ID\")).head()[0]\n",
    "    return print(\"Total: {}\\nDistinct: {}\".format(total, distinct))\n",
    "\n",
    "def cnt_cols(y):\n",
    "    for column in y.columns:\n",
    "        print(column + \": \" + str(y.filter(col(column).isNotNull()).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функции для подсчёта соотношения: кол-во ид по документам\n",
    "def cnt_table(x):\n",
    "    cnt = x.groupby(\"document_class_id\").agg(countDistinct(\"ID\"))\n",
    "    return cnt.show()\n",
    "\n",
    "def cnt_meta(y):\n",
    "    cnt = y.groupby(\"document_class_id\").agg(countDistinct(\"ID\"))\n",
    "    return cnt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_table(proc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_meta(proc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_ids(proc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_cols(proc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min и max даты в паркете\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "proc_df.agg(F.min(\"DOCUMENT_CREATED_DATE\"), F.max(\"DOCUMENT_CREATED_DATE\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project parameters\n",
    "ui_enable = 'false'\n",
    "project_name = 'statistics'\n",
    "#root_path = '/project'\n",
    "root_path = '/user/mivanovskij'\n",
    "project_path = root_path + '/' + project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark context is not running\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext, SparkSession, HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from datetime import datetime as dttm\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# config processing\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print('Spark context is not running')\n",
    "\n",
    "# context config\n",
    "jar_paths = ['/etc/jdbc/*']\n",
    "conf = (pyspark.SparkConf()\\\n",
    "            .set('spark.dynamicAllocation.maxExecutors','200')\n",
    "            .set('spark.driver.extraClassPath', ':'.join(jar_paths))\n",
    "            .set('spark.executor.extraClassPath', ':'.join(jar_paths))\n",
    "            .set('spark.jars', ','.join(jar_paths))\n",
    "            .set('spark.ui.enabled', ui_enable) #Выключить при установке на регламент\n",
    "            .set('spark.executor.memory','8g')\n",
    "            .set('spark.driver.memory', '8g')\n",
    "            .set('spark.scheduler.mode','FAIR')\n",
    "            .set('spark.driver.maxResultSize', '8g')\n",
    "            .set('spark.rpc.message.maxSize','256')\n",
    "            .set('spark.sql.execution.arrow.enabled', 'true')\n",
    "           )\n",
    "    \n",
    "# Config initialization\n",
    "sc = pyspark.SparkContext(appName=project_name, conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "sql = sqlContext.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скрипт для юниона датафреймов с разным кол-вом столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|null|   5|   6|   7|\n",
      "|null|   4|   5|   6|\n",
      "|   0|   1|   2|   3|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df1 = sqlContext.createDataFrame([[0, 1, 2, 3], [None, 4, 5, 6]], [\"col0\", \"col1\", \"col2\", \"col3\"])\n",
    "df2 = sqlContext.createDataFrame([[4, 5, 6], [5, 6, 7]], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "not_in_df1 = set(df2.columns) - set(df1.columns)\n",
    "for col in not_in_df1:\n",
    "    df1 = df1.withColumn(col, lit(None).cast(StringType()))\n",
    "\n",
    "not_in_df2 = set(df1.columns) - set(df2.columns)\n",
    "for col in not_in_df2:\n",
    "    df2 = df2.withColumn(col, lit(None).cast(StringType()))\n",
    "\n",
    "df1.unionByName(df2).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def UnionFunc(a,b):\n",
    "    not_in_a = set(b.columns) - set(a.columns)\n",
    "    for col in not_in_a:\n",
    "        a = a.withColumn(col, lit(None).cast(StringType()))\n",
    "\n",
    "    not_in_b = set(a.columns) - set(b.columns)\n",
    "    for col in not_in_b:\n",
    "        b = b.withColumn(col, lit(None).cast(StringType()))\n",
    "    \n",
    "    return a.unionByName(b).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sqlContext.createDataFrame([[0, 1, 2, 3], [None, 4, 5, 6]], [\"col0\", \"col1\", \"col2\", \"col3\"])\n",
    "df2 = sqlContext.createDataFrame([[4, 5, 6], [5, 6, 7]], [\"col1\", \"col2\", \"col3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df = UnionFunc(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|null|   5|   6|   7|\n",
      "|null|   4|   5|   6|\n",
      "|   0|   1|   2|   3|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = sqlContext.createDataFrame([[1, 2, 3], [4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
    "df22 = sqlContext.createDataFrame([[4, 5, 6], [5, 6, 7]], [\"col1\", \"col2\", \"col3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df_1 = UnionFunc(df12,df22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   5|   6|   7|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скрипт для проверки полноты данных в витрине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project parameters\n",
    "ui_enable = 'false'\n",
    "project_name = 'check_completeness'\n",
    "log_fl = None\n",
    "#root_path = '/project'\n",
    "root_path = '/user/mivanovskij'\n",
    "project_path = root_path + '/' + project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read table-config\n",
    "params_greenplum1 = {\n",
    "    'user': username,\n",
    "    'password':password,\n",
    "    'url':f'jdbc:postgresql://{address}:{port}/{database}',\n",
    "    'driver':'org.postgresql.Driver',\n",
    "    'dbtable' : '(select distinct xpath_clear, max_nn_cnt, attr, document_class_id, part_num from common_analytics2.dict_to_dispensary_observation_refuse) as foo'\n",
    "}\n",
    "\n",
    "log_msg(log_fl, '<<< Getting dictionary from greenplum >>>')\n",
    "\n",
    "xpath_dict = sqlContext.read\\\n",
    "                       .format('jdbc')\\\n",
    "                       .options(**params_greenplum1)\\\n",
    "                       .load()\n",
    "log_msg(log_fl, '<<< Getting dictionary from greenplum ends >>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 12:44:59,976] {PapermillOperator} INFO - <<< Tagvalue loading starts >>>\n",
      "[2023-06-01 12:45:34,661] {PapermillOperator} INFO - <<< Tagvalue loading done >>>\n"
     ]
    }
   ],
   "source": [
    "#Подтянем tagvalue\n",
    "log_msg(log_fl, '<<< Tagvalue loading starts >>>')\n",
    "tv = sqlContext.read.option(\"mergeSchema\", \"true\").parquet('/ods/simi_docs/xml/docs_tagvalue')\n",
    "log_msg(log_fl, '<<< Tagvalue loading done >>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подтянем витрину\n",
    "df_main = sqlContext.read.parquet('/user/MIvanovskij/do_refuse/mart_final') #поменять\n",
    "df_ccts = df_main.select(\"document_class_id\").where(f\"document_class_id is not null\").distinct()\n",
    "df_ccts = df_ccts.rdd.map(lambda x: x.document_class_id).collect()\n",
    "df_ccts = tuple(set(df_ccts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('85066', '41062', '96511')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ccts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ccts = df_ccts.rdd.map(lambda x: x.document_class_id).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Уберём лишние cct\n",
    "tv_cut = tv.where(f\"document_class_id in {df_ccts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOCUMENT_CREATED_DATE',\n",
       " 'ID',\n",
       " 'DOCUMENT_CLASS_ID',\n",
       " 'root_tag',\n",
       " 'nn1',\n",
       " 'patient_id',\n",
       " 'event_id',\n",
       " 'start_time',\n",
       " 'uid',\n",
       " 'mo_type',\n",
       " 'mo_parent_id',\n",
       " 'mo_id',\n",
       " 'mo_name',\n",
       " 'mu_id',\n",
       " 'mu_name',\n",
       " 'mo_municipality',\n",
       " 'mo_street',\n",
       " 'mo_house',\n",
       " 'mo_corp',\n",
       " 'mo_building',\n",
       " 'medic_mo',\n",
       " 'func_value',\n",
       " 'medic_position',\n",
       " 'medic_division',\n",
       " 'composer_name',\n",
       " 'diagnos_code',\n",
       " 'diagnos_mkb',\n",
       " 'refuse_date',\n",
       " 'refuse_reason',\n",
       " 'dispensary_group']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def cnt_cols(y):\n",
    "    for column in y.columns:\n",
    "        print(column + \": \" + str(y.filter(col(column).isNotNull()).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT_CREATED_DATE: 5860761\n",
      "ID: 5860761\n",
      "DOCUMENT_CLASS_ID: 5860761\n",
      "root_tag: 5860761\n",
      "nn1: 5860761\n",
      "patient_id: 5860761\n",
      "event_id: 5860761\n",
      "start_time: 5860761\n",
      "uid: 5860749\n",
      "mo_type: 5860761\n",
      "mo_parent_id: 2404761\n",
      "mo_id: 2404761\n",
      "mo_name: 2404761\n",
      "mu_id: 3456000\n",
      "mu_name: 3456000\n",
      "mo_municipality: 0\n",
      "mo_street: 0\n",
      "mo_house: 0\n",
      "mo_corp: 0\n",
      "mo_building: 0\n",
      "medic_mo: 5860761\n",
      "func_value: 5860761\n",
      "medic_position: 5860761\n",
      "medic_division: 5860761\n",
      "composer_name: 5860761\n",
      "diagnos_code: 5860761\n",
      "diagnos_mkb: 5860761\n",
      "refuse_date: 5860759\n",
      "refuse_reason: 5860759\n",
      "dispensary_group: 5851064\n"
     ]
    }
   ],
   "source": [
    "cnt_cols(df_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/User/Downloads/Блок3_Данные.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in '/home/MIvanovskij': ['patronazh_mart', 'despensary', 'ii', 'airflow_process.py', '.profile', 'tumor_monitoring', '.config', 'antropo_mart', 'dispensary_new', 'row_count_onco.ipynb', 'maltur', 'malignant_tumors', 'drugs_mart', '.virtual_documents', 'DN_refuse.ipynb', 'ii.ipynb', '.jupyter', 'diag_new.ipynb', 'lin_reg.ipynb', 'onco_consilium', 'drugs_mart_1', 'write_vault_token (1).ipynb', 'Untitled.ipynb', 'bigdata1381', 'Procedure_end.ipynb', 'oncology.ipynb', 'e-prescriptions', 'tap', '.fonts', '.emacs', 'health_group', 'spark-warehouse', '.cache', 'e-prescriptions_1', 'write_vault_token(2).ipynb', 'tv_eris.ipynb', 'tumor_mon', 'stat.ipynb', 'bad_habits.ipynb', 'main.ipynb', '.viminfo', 'dispanser_adult', 'lab_research', 'refer_hospitalization', 'dispensary_observation_refuse', 'allergic_anamnesis (2).ipynb', 'codes_classifier', 'seaborn-data', 'onco_consilium_1', 'Dn_new.ipynb', 'vaccination', 'tumor_mon_1', 'tap_mart', 'patronazh_qa.ipynb', 'disp_observ.ipynb', 'Untitled2.ipynb', '.inputrc', '01_prescribing_drugs_simi.ipynb', 'Drugs_1', 'bigdata_1910_simi_lab_research.ipynb', 'patronazh_nr.ipynb', 'bad_habits', 'BIGDATA_2657_SICK_LIST', '.bashrc', 'e-prescr (1).ipynb', 'vaccination.ipynb', '.lesshst', 'transform_list.ipynb', '.ipynb_checkpoints', '.ipython', '057y', 'write_vault_token (3).ipynb', 'train_test_notebook.ipynb', 'procedure_new', 'antropo', '.bash_history', '122435', '.local']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
    "files = os.listdir(cwd)  # Get all the files in that directory\n",
    "print(\"Files in %r: %s\" % (cwd, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python383-Spark4Users",
   "language": "python",
   "name": "python383-spark-users"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
