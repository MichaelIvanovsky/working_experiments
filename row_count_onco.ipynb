{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldap_username = 'malininpa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark Session...\n",
      "Spark Session row_count started\n",
      "metadata <= ods_simi_ora_emias_simi_v2.metadata (hdp_active_flg==1)\n",
      "tags <= ods_simi_ora_emias_simi_v2.document_tag (hdp_active_flg==1)\n",
      "class <= ods_simi_ora_emias_simi_v2.document_class (hdp_active_flg==1)\n",
      "association <= ods_simi_ora_emias_simi_v2.document_association (hdp_active_flg==1)\n",
      "tagvalue <= ods_simi_docs_xml.docs_tagvalue\n",
      "Авторизация через персональный SECRET_TOKEN\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import subprocess\n",
    "\n",
    "sys.path.append('/home/shared/')\n",
    "\n",
    "from common.spark import Spark\n",
    "from common.database import Database\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "\n",
    "jar_paths = ['/etc/jdbc/*']\n",
    "\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "project_name = \"row_count_onco\"\n",
    "feature_name = \"row_count_onco\"\n",
    "schema_name = \"common_analytics2\" # ввести свою схему\n",
    "project_path = f\"/user/{ldap_username}/{feature_name}\" \n",
    "\n",
    "sc = Spark(project_name)\n",
    "\n",
    "# инициализация спарковского окружения\n",
    "\n",
    "queue = None\n",
    "ui_enable = 'true'\n",
    "log_fl = None\n",
    "\n",
    "\n",
    "spark_config = {\n",
    "    \"spark.dynamicAllocation.maxExecutors\": \"1000\",\n",
    "    'spark.driver.extraClassPath': ':'.join(jar_paths),\n",
    "    'spark.executor.extraClassPath': ':'.join(jar_paths),\n",
    "    'spark.jars': ','.join(jar_paths),\n",
    "    'spark.ui.enabled': ui_enable,\n",
    "    'spark.executor.memory': '32g',\n",
    "    'spark.driver.memory': '32g',\n",
    "    'spark.scheduler.mode': 'FAIR',\n",
    "    'spark.driver.maxResultSize': '16g',\n",
    "    'spark.yarn.queue': queue\n",
    "}\n",
    "spark = sc.run(\"loaded_ui\", config=spark_config)  # simple or loaded\n",
    "sc.load_simi()  # loads - пока не нужно\n",
    "\n",
    "# подключение к гринпламу (либа sqlalchemy)\n",
    "gp = Database('gp_etl', mode='token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cct_to_cnt = (56080, 81051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select a.*\n",
    "from tagvalue a\n",
    "where true\n",
    "and document_class_id in {cct_to_cnt}\n",
    "\"\"\").write.mode('overwrite').parquet(os.path.join(project_path, 'tagvalue_custom'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select a.*\n",
    "from tagvalue a\n",
    "where true\n",
    "and id = 'e10399d7-6f37-4ceb-8e32-157b84720169'\n",
    "and document_created_date = '2022-09-05'\n",
    "and document_class_id in {cct_to_cnt}\n",
    "\"\"\").write.mode('overwrite').parquet(os.path.join(project_path, 'tagvalue_one_doc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet').load(os.path.join(project_path, 'tagvalue_custom'))\n",
    "df.createOrReplaceTempView('df')\n",
    "df = df.toDF(*[c.lower() for c in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "df_agg = (df\n",
    "          .groupBy('xpath', 'id')\n",
    "          .agg(\n",
    "              F.count('id').alias('cnt')\n",
    "          )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_agg.withColumn('xpath_star', F.regexp_replace('xpath', r'[^\\]]+/|[^\\]]+$', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates(['id', 'xpath_star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('bracket_cnt', size(split(col('xpath_star'), r\"\\[\")) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('xpath_base', F.regexp_extract('xpath_star', r'^(.*?)\\[', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.xpath_base != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_bracket_cnt = (df\n",
    "          .groupBy('xpath_base', 'id')\n",
    "          .agg(\n",
    "              F.max('bracket_cnt').alias('max_bracket_cnt')\n",
    "          )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_bracket_cnt = (\n",
    "    df_max_bracket_cnt\n",
    "    .withColumnRenamed('xpath_base', 'xpath_base_max')\n",
    "    .withColumnRenamed('id', 'id_max')    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_max_bracket_cnt, (df.id == df_max_bracket_cnt.id_max) & (df.xpath_base == df_max_bracket_cnt.xpath_base_max) & (df.bracket_cnt == df_max_bracket_cnt.max_bracket_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\n",
    "          .groupBy('xpath_base', 'id')\n",
    "          .agg(\n",
    "              F.count('xpath_star').alias('cnt')\n",
    "          )\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "onco_consilium_row_count = (df\n",
    "          .groupBy('id')\n",
    "          .agg(\n",
    "              floor(round(exp(F.sum(log('cnt'))))).alias('product')\n",
    "          )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = 'common_analytics2'\n",
    "table_name = 'onco_consilium_row_count'\n",
    "project_path = os.path.join(f\"/user/{ldap_username}/{schema_name}\", table_name)\n",
    "                             \n",
    "onco_consilium_row_count.write.parquet(project_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_external_table_ddl(path, table):\n",
    "    tpl = \"\"\"DROP EXTERNAL TABLE IF EXISTS {table};\n",
    "    CREATE EXTERNAL TABLE {table} ( \n",
    "    {fields} \n",
    "    )\n",
    "    LOCATION ('pxf:///{path}?PROFILE=hdfs:parquet') ON ALL\n",
    "    FORMAT 'CUSTOM' ( FORMATTER='pxfwritable_import' )\n",
    "    ENCODING 'UTF8';\"\"\"\n",
    "#     spark = sc.spark\n",
    "#     df = spark.read.parquet(path)\n",
    "    df = spark.read.format('parquet').load(path)\n",
    "    dict_types = {\"string\": \"text\", \"int\": \"integer\"}\n",
    "    types =list(map(lambda x: (x[0],dict_types[x[1]] if x[1] in dict_types.keys() else x[1]), df.dtypes))\n",
    "    fields = ',\\n'.join([f\"{i[0]} {i[1]}\" for i in types])\n",
    "    return tpl.format(table=table, fields=fields, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/malininpa/common_analytics2/onco_consilium_row_count'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = get_external_table_ddl(project_path, f'{schema_name}.{table_name}')\n",
    "q = q + f\" GRANT ALL PRIVILEGES ON {schema_name}.{table_name} TO gpuser;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gp.con.connect().execution_options(autocommit=True) as conn:\n",
    "    conn.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python383-Spark4Users",
   "language": "python",
   "name": "python383-spark-users"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
